{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNzSMho6N6XR2WVyjqq93ZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kartikey0412/google_colab/blob/main/cnn_lstm_gru_intro_imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZgEAn041n9-"
      },
      "source": [
        "%matplotlib inline\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.utils import np_utils\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
        "from keras.layers import TimeDistributed, Activation, SimpleRNN, GRU\n",
        "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
        "from keras.regularizers import l2, l1\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.pooling import *\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.metrics import categorical_crossentropy, categorical_accuracy\n",
        "from keras.layers.convolutional import *\n",
        "from keras.preprocessing import image, sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras import applications"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FLWE8B12VLP"
      },
      "source": [
        "from keras.datasets import imdb as keras_imdb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAXbRnwA2jud",
        "outputId": "b8189fdc-1cb6-4c61-ff60-c8cb8019b1bd"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras_imdb.load_data(num_words=None,\n",
        "                                                       skip_top=0,\n",
        "                                                       maxlen=None,\n",
        "                                                       seed=113,\n",
        "                                                       start_char=1,\n",
        "                                                       oov_char=2,\n",
        "                                                       index_from=3)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFm0Ec0L4yzj",
        "outputId": "6ea875e1-fcbb-4638-c652-76eb940d3504"
      },
      "source": [
        "print(x_train[0:5][:10])\n",
        "print(len(x_train[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
            " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
            " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
            " list([1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 15305, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 64317, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 33929, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 23005, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 87564, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 18435, 192, 5, 1219, 3890, 19, 20523, 217, 4122, 1710, 537, 20341, 1236, 5, 736, 10, 10, 61, 403, 9, 47289, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 18665, 594, 7, 5168, 94, 9096, 3987, 15242, 11, 28280, 4, 538, 7, 1795, 246, 56615, 9, 10161, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 10197, 19, 49, 7, 4, 1885, 13699, 1118, 25, 80, 126, 842, 10, 10, 47289, 18223, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 16376, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 25837, 21, 29, 9, 2841, 23, 4, 1010, 26747, 793, 6, 13699, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 16376, 8, 124, 4, 882, 4, 882, 496, 27, 33029, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 11120, 1311, 8, 4, 23643, 7, 31, 7, 29851, 91, 22793, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 11876, 1775, 3353, 12716, 1846, 4, 11286, 7, 154, 5, 4, 518, 53, 13243, 11286, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 18223, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 33058, 4, 22793, 10359, 9, 242, 4, 91, 1202, 11377, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 18223, 13, 188, 1076, 3222, 19, 4, 13465, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 18223, 13, 33195, 14, 280, 13, 219, 4, 52788, 431, 758, 859, 4, 953, 1052, 12283, 7, 5991, 5, 94, 40, 25, 238, 60, 35410, 4, 15812, 804, 27767, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 22148, 23, 12, 1685, 195, 25, 238, 60, 796, 13713, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574])\n",
            " list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 22016, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113])]\n",
            "218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhztN3n85C0S",
        "outputId": "89c43f6c-eb3d-4f9e-c003-405883bd4feb"
      },
      "source": [
        "word_to_id = keras.datasets.imdb.get_word_index()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpZl6Yo66xgV",
        "outputId": "63a823af-bb41-4216-cc16-251c65408525"
      },
      "source": [
        "word_to_id[\"horrible\"]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "524"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bXTbAPbne7g",
        "outputId": "bafb030c-a3db-4375-f721-6cad417a38ce"
      },
      "source": [
        "list(word_to_id.items())[:4]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fawn', 34701), ('tsukino', 52006), ('nunnery', 52007), ('sonja', 16816)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91lRCCdunnMy"
      },
      "source": [
        "# Add terms to our vocabulary\n",
        "word_to_id = {word : (word_id + 3) for word, word_id in keras.datasets.imdb.get_word_index().items()}  # Add an offset of 3 to leave room for the new terms\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "\n",
        "# Also define the opposite mapping\n",
        "id_to_word = {word_id: word for word, word_id in word_to_id.items()}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_k-66K7ol7O"
      },
      "source": [
        "def get_review(x, i):\n",
        "    return \" \".join(id_to_word[id_] for id_ in x[i])\n",
        "\n",
        "def get_label(y, i):\n",
        "    if y[i] == 1:\n",
        "        return \"Positive\"\n",
        "    else:\n",
        "        return \"Negative\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnMmZJhnowHv",
        "outputId": "e770f477-d237-4ad7-8ef9-60cf3339d292"
      },
      "source": [
        "for review_index in range(10):\n",
        "    review = get_review(x_train, review_index)\n",
        "    label = get_label(y_train, review_index)\n",
        "    print(\"*\" * 50)\n",
        "    print(f\"Review index: {review_index}\")\n",
        "    print(f\"Label: {label}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(review)\n",
        "    print(\"*\" * 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "Review index: 0\n",
            "Label: Positive\n",
            "--------------------------------------------------\n",
            "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
            "**************************************************\n",
            "**************************************************\n",
            "Review index: 1\n",
            "Label: Negative\n",
            "--------------------------------------------------\n",
            "<START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal measures the hair is big lots of boobs bounce men wear those cut tee shirts that show off their stomachs sickening that men actually wore them and the music is just synthesiser trash that plays over and over again in almost every scene there is trashy music boobs and paramedics taking away bodies and the gym still doesn't close for bereavement all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\n",
            "**************************************************\n",
            "**************************************************\n",
            "Review index: 2\n",
            "Label: Negative\n",
            "--------------------------------------------------\n",
            "<START> this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had earnt working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how embarrasing this is to watch save yourself an hour a bit of your life\n",
            "**************************************************\n",
            "**************************************************\n",
            "Review index: 3\n",
            "Label: Positive\n",
            "--------------------------------------------------\n",
            "<START> the scots excel at storytelling the traditional sort many years after the event i can still see in my mind's eye an elderly lady my friend's mother retelling the battle of culloden she makes the characters come alive her passion is that of an eye witness one to the events on the sodden heath a mile or so from where she lives br br of course it happened many years before she was born but you wouldn't guess from the way she tells it the same story is told in bars the length and breadth of scotland as i discussed it with a friend one night in mallaig a local cut in to give his version the discussion continued to closing time br br stories passed down like this become part of our being who doesn't remember the stories our parents told us when we were children they become our invisible world and as we grow older they maybe still serve as inspiration or as an emotional reservoir fact and fiction blend with aspiration role models warning stories archetypes magic and mystery br br my name is aonghas like my grandfather and his grandfather before him our protagonist introduces himself to us and also introduces the story that stretches back through generations it produces stories within stories stories that evoke the impenetrable wonder of scotland its rugged mountains shrouded in mists the stuff of legend yet seach'd is rooted in reality this is what gives it its special charm it has a rough beauty and authenticity tempered with some of the finest gaelic singing you will ever hear br br aonghas angus visits his grandfather in hospital shortly before his death he burns with frustration part of him yearns to be in the twenty first century to hang out in glasgow but he is raised on the western shores among a gaelic speaking community br br yet there is a deeper conflict within him he yearns to know the truth the truth behind his grandfather's ancient stories where does fiction end and he wants to know the truth behind the death of his parents br br he is pulled to make a last fateful journey to the summit of one of scotland's most inaccessible mountains can the truth be told or is it all in stories br br in this story about stories we revisit bloody battles poisoned lovers the folklore of old and the sometimes more treacherous folklore of accepted truth in doing so we each connect with angus as he lives the story of his own life br br seachd the inaccessible pinnacle is probably the most honest unpretentious and genuinely beautiful film of scotland ever made like angus i got slightly annoyed with the pretext of hanging stories on more stories but also like angus i forgave this once i saw the 'bigger picture ' forget the box office pastiche of braveheart and its like you might even forego the justly famous dramatisation of the wicker man to see a film that is true to scotland this one is probably unique if you maybe meditate on it deeply enough you might even re evaluate the power of storytelling and the age old question of whether there are some truths that cannot be told but only experienced\n",
            "**************************************************\n",
            "**************************************************\n",
            "Review index: 4\n",
            "Label: Negative\n",
            "--------------------------------------------------\n",
            "<START> worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of my friends were asleep and i was still suffering worst plot worst script worst movie i have ever seen i wanted to hit my head up against a wall for an hour then i'd stop and you know why because it felt damn good upon bashing my head in i stuck that damn movie in the microwave and watched it burn and that felt better than anything else i've ever done it took american psycho army of darkness and kill bill just to get over that crap i hate you sandler for actually going through with this and ruining a whole day of my life\n",
            "**************************************************\n",
            "**************************************************\n",
            "Review index: 5\n",
            "Label: Negative\n",
            "--------------------------------------------------\n",
            "<START> begins better than it ends funny that the russian submarine crew outperforms all other actors it's like those scenes where documentary shots br br spoiler part the message dechifered was contrary to the whole story it just does not mesh br br\n",
            "**************************************************\n",
            "**************************************************\n",
            "Review index: 6\n",
            "Label: Positive\n",
            "--------------------------------------------------\n",
            "<START> lavish production values and solid performances in this straightforward adaption of jane austen's satirical classic about the marriage game within and between the classes in provincial 18th century england northam and paltrow are a salutory mixture as friends who must pass through jealousies and lies to discover that they love each other good humor is a sustaining virtue which goes a long way towards explaining the accessability of the aged source material which has been toned down a bit in its harsh scepticism i liked the look of the film and how shots were set up and i thought it didn't rely too much on successions of head shots like most other films of the 80s and 90s do very good results\n",
            "**************************************************\n",
            "**************************************************\n",
            "Review index: 7\n",
            "Label: Negative\n",
            "--------------------------------------------------\n",
            "<START> the hamiltons tells the story of the four hamilton siblings teenager francis cory knauf twins wendell joseph mckelheer darlene mackenzie firgens the eldest david samuel who is now the surrogate parent in charge the hamilton's move house a lot franics is unsure why is unhappy with the way things are the fact that his brother's sister kidnap imprison murder people in the basement doesn't help relax or calm francis' nerves either francis know's something just isn't right when he eventually finds out the truth things will never be the same again br br co written co produced directed by mitchell altieri phil flores as the butcher brothers who's only other film director's credit so far is the april fool's day 2008 remake enough said this was one of the 'films to die for' at the 2006 after dark horrorfest or whatever it's called in keeping with pretty much all the other's i've seen i thought the hamiltons was complete total utter crap i found the character's really poor very unlikable the slow moving story failed to capture my imagination or sustain my interest over it's 85 a half minute too long 86 minute duration the there's the awful twist at the end which had me laughing out loud there's this really big sustained build up to what's inside a cupboard thing in the hamiltons basement it's eventually revealed to be a little boy with a teddy is that really supposed to scare us is that really supposed to shock us is that really something that is supposed to have us talking about it as the end credits roll is a harmless looking young boy the best 'twist' ending that the makers could come up with the boring plot plods along it's never made clear where the hamiltons get all their money from to buy new houses since none of them seem to work except david in a slaughterhouse i doubt that pays much or why they haven't been caught before now the script tries to mix in every day drama with potent horror it just does a terrible job of combining the two to the extent that neither aspect is memorable or effective a really bad film that i am struggling to say anything good about br br despite being written directed by the extreme sounding butcher brothers there's no gore here there's a bit of blood splatter a few scenes of girls chained up in a basement but nothing you couldn't do at home yourself with a bottle of tomato ketchup a camcorder the film is neither scary since it's got a very middle class suburban setting there's zero atmosphere or mood there's a lesbian suggest incestuous kiss but the hamiltons is low on the exploitation scale there's not much here for the horror crowd br br filmed in petaluma in california this has that modern low budget look about it it's not badly made but rather forgettable the acting by an unknown to me cast is nothing to write home about i can't say i ever felt anything for anyone br br the hamiltons commits the cardinal sin of being both dull boring from which it never recovers add to that an ultra thin story no gore a rubbish ending character's who you don't give a toss about you have a film that did not impress me at all\n",
            "**************************************************\n",
            "**************************************************\n",
            "Review index: 8\n",
            "Label: Positive\n",
            "--------------------------------------------------\n",
            "<START> just got out and cannot believe what a brilliant documentary this is rarely do you walk out of a movie theater in such awe and amazement lately movies have become so over hyped that the thrill of discovering something truly special and unique rarely happens amores perros did this to me when it first came out and this movie is doing to me now i didn't know a thing about this before going into it and what a surprise if you hear the concept you might get the feeling that this is one of those touchy movies about an amazing triumph covered with over the top music and trying to have us fully convinced of what a great story it is telling but then not letting us in fortunetly this is not that movie the people tell the story this does such a good job of capturing every moment of their involvement while we enter their world and feel every second with them there is so much beyond the climb that makes everything they go through so much more tense touching the void was also a great doc about mountain climbing and showing the intensity in an engaging way but this film is much more of a human story i just saw it today but i will go and say that this is one of the best documentaries i have ever seen\n",
            "**************************************************\n",
            "**************************************************\n",
            "Review index: 9\n",
            "Label: Negative\n",
            "--------------------------------------------------\n",
            "<START> this movie has many problem associated with it that makes it come off like a low budget class project from someone in film school i have to give it credit on its campiness though many times throughout the movie i found myself laughing hysterically it was so bad at times that it was comical which made it a fun watch br br if you're looking for a low grade slasher movie with a twist of psychological horror and a dash of campy ridiculousness then pop a bowl of popcorn invite some friends over and have some fun br br i agree with other comments that the sound is very bad dialog is next to impossible to follow much of the time and the soundtrack is kind of just there\n",
            "**************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhExfTJ6rp85"
      },
      "source": [
        "## Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ7C4wNorueP"
      },
      "source": [
        "vocab_size = 5000\n",
        "trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\n",
        "test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWvBcsuD-YGS",
        "outputId": "54b3d51e-0417-445e-ce1f-9b72fc73a937"
      },
      "source": [
        "lens = np.array(list(map(len, trn)))\n",
        "print('Maximum text length:', lens.max(),' -- Minimum length:', lens.min(), '-- Mean length of text:',lens.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum text length: 2494  -- Minimum length: 11 -- Mean length of text: 238.71364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW_q18xO-slI"
      },
      "source": [
        "### Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un2HAKXg-uDV"
      },
      "source": [
        "# we'll pad all inputs to obtain homogeneous inputs of dim 500\n",
        "seq_len = 500\n",
        "\n",
        "trn = sequence.pad_sequences(trn, maxlen = seq_len,value=0, padding=\"post\", truncating=\"post\")\n",
        "test = sequence.pad_sequences(test, maxlen = seq_len,value=0, padding=\"post\", truncating=\"post\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "OJmptc8e_lzo",
        "outputId": "d4e3241b-2329-435f-a1d7-e867ef18f345"
      },
      "source": [
        "print(trn.shape)\n",
        "get_review(trn, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 500)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> big hair big orange bad music and a giant safety orange these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an orange the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are orange and funny in equal orange the hair is big lots of orange orange men wear those cut orange orange that show off their orange orange that men actually wore them and the music is just orange trash that plays over and over again in almost every scene there is trashy music orange and orange taking away bodies and the orange still doesn't close for orange all orange aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXy0LwqR0Lo9"
      },
      "source": [
        "# Modeling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BDPiUnN0bqU"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgH0NbQj0dvA",
        "outputId": "db42c82a-aac2-4d22-bc2a-af86c7a025de"
      },
      "source": [
        "def MLP():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size,32,input_length=seq_len))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(100,activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Dense(100,activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = MLP()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 16000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               1600100   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,771,101\n",
            "Trainable params: 1,770,701\n",
            "Non-trainable params: 400\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MraVqwdu0kRC",
        "outputId": "17b111c4-e570-45dd-d40e-ed3ea66bf0c8"
      },
      "source": [
        "model.fit(trn, y_train, validation_data=(test, y_test), epochs=10, batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 2s 37ms/step - loss: 0.1479 - accuracy: 0.9478 - val_loss: 0.4276 - val_accuracy: 0.8331\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 2s 35ms/step - loss: 0.0757 - accuracy: 0.9771 - val_loss: 0.3874 - val_accuracy: 0.8302\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 2s 35ms/step - loss: 0.0420 - accuracy: 0.9898 - val_loss: 0.3637 - val_accuracy: 0.8397\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 2s 35ms/step - loss: 0.0208 - accuracy: 0.9964 - val_loss: 0.3996 - val_accuracy: 0.8268\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.0141 - accuracy: 0.9972 - val_loss: 0.4357 - val_accuracy: 0.8366\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.0079 - accuracy: 0.9990 - val_loss: 0.4691 - val_accuracy: 0.8368\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.0064 - accuracy: 0.9991 - val_loss: 0.5350 - val_accuracy: 0.8359\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 2s 35ms/step - loss: 0.0047 - accuracy: 0.9994 - val_loss: 0.5997 - val_accuracy: 0.8377\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.0033 - accuracy: 0.9998 - val_loss: 0.6380 - val_accuracy: 0.8388\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.0027 - accuracy: 0.9998 - val_loss: 0.6700 - val_accuracy: 0.8359\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdac7603f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaGn4SNL43fD",
        "outputId": "ed001f10-9ecc-4bce-a069-adb1235f69c3"
      },
      "source": [
        "scores = model.evaluate(test,y_test,verbose=0)\n",
        "print('loss: ', scores[0],'- accuracy: ', scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss:  0.6699978113174438 - accuracy:  0.8358799815177917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Crj4d-bM5TcK"
      },
      "source": [
        "## 1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U08TRtBn5UHE",
        "outputId": "42f49029-f699-436a-ced7-4762712e9fe7"
      },
      "source": [
        "def CNN():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 32, input_length=seq_len))\n",
        "    \n",
        "    #model.add(Dropout(0.2))\n",
        "    model.add(Conv1D(64, 5, padding='same', activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling1D())\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    \n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = CNN()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 500, 64)           10304     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 500, 64)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 250, 64)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 16000)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               1600100   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,770,505\n",
            "Trainable params: 1,770,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqZo8oQz54zN",
        "outputId": "15655971-5c5d-4183-94ea-5ff6408d9681"
      },
      "source": [
        "model.fit(trn, y_train, validation_data=(test, y_test), epochs=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "782/782 [==============================] - 13s 10ms/step - loss: 0.5232 - accuracy: 0.6959 - val_loss: 0.2769 - val_accuracy: 0.8844\n",
            "Epoch 2/4\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.2123 - accuracy: 0.9218 - val_loss: 0.2812 - val_accuracy: 0.8819\n",
            "Epoch 3/4\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.1555 - accuracy: 0.9444 - val_loss: 0.2964 - val_accuracy: 0.8835\n",
            "Epoch 4/4\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.1108 - accuracy: 0.9621 - val_loss: 0.3689 - val_accuracy: 0.8780\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdac6208150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf86wpNM6V7l",
        "outputId": "eec91fdd-d247-4c1a-ad13-a71b215652b8"
      },
      "source": [
        "scores = model.evaluate(test,y_test,verbose=0)\n",
        "print('loss: ', scores[0],'- accuracy: ', scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss:  0.36885955929756165 - accuracy:  0.8780400156974792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVWyE9jvQs35"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG2k0E_f7ayg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "258d0a77-e3ab-495c-9782-4f39a0598e2c"
      },
      "source": [
        "def RNN_LSTM():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size,5,input_length=seq_len))\n",
        "    model.add(LSTM(50))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = RNN_LSTM() "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 500, 5)            25000     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50)                11200     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 36,251\n",
            "Trainable params: 36,251\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qWvhp4eRBL5",
        "outputId": "0c20f127-f3fe-4d30-acfa-85cea7aff8c7"
      },
      "source": [
        "model.fit(trn, y_train, validation_data=(test, y_test), epochs=4, batch_size=512)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "49/49 [==============================] - 11s 69ms/step - loss: 0.6932 - accuracy: 0.4938 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 2/4\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 0.6929 - accuracy: 0.5038 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 3/4\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 0.7082 - accuracy: 0.5121 - val_loss: 0.6938 - val_accuracy: 0.5070\n",
            "Epoch 4/4\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 0.6924 - accuracy: 0.5144 - val_loss: 0.6929 - val_accuracy: 0.5056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8657721f90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwMxqqUURE4G",
        "outputId": "2cc0042d-58ac-4130-938b-c9b7aba35ba6"
      },
      "source": [
        "scores = model.evaluate(test,y_test,verbose=0, batch_size=512)\n",
        "print('loss: ', scores[0],'- accuracy: ', scores[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss:  0.6929231286048889 - accuracy:  0.5055599808692932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJKemTI6RfYK"
      },
      "source": [
        "## GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW3NxHs8RP62",
        "outputId": "7486fd9e-11a1-4e89-a0ea-b7a635f95cdd"
      },
      "source": [
        "def RNN_GRU():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size,5,input_length=seq_len))\n",
        "    model.add(GRU(50))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = RNN_GRU()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 5)            25000     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 50)                8550      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 33,601\n",
            "Trainable params: 33,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKZRVqCgRh6B",
        "outputId": "4b811faf-9d4b-4542-97e9-aabb6bac90f8"
      },
      "source": [
        "model.fit(trn, y_train, validation_data=(test, y_test), epochs=4, batch_size=512)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "49/49 [==============================] - 4s 63ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5050\n",
            "Epoch 2/4\n",
            "49/49 [==============================] - 3s 55ms/step - loss: 0.6929 - accuracy: 0.5030 - val_loss: 0.6930 - val_accuracy: 0.5050\n",
            "Epoch 3/4\n",
            "49/49 [==============================] - 3s 55ms/step - loss: 0.6924 - accuracy: 0.5107 - val_loss: 0.6929 - val_accuracy: 0.5052\n",
            "Epoch 4/4\n",
            "49/49 [==============================] - 3s 55ms/step - loss: 0.6917 - accuracy: 0.5101 - val_loss: 0.6934 - val_accuracy: 0.5001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f865fdcf490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G70z2plRmAC",
        "outputId": "fb5e9907-63b6-4e45-96ef-7399f6b550ee"
      },
      "source": [
        "scores = model.evaluate(test,y_test,verbose=0)\n",
        "print('loss: ', scores[0],'- accuracy: ', scores[1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss:  0.693374752998352 - accuracy:  0.5001199841499329\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}